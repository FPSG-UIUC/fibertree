{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import copy\n",
    "import itertools\n",
    "from fibertree import Payload, Fiber, CoordPayload, Tensor, TensorImage, TensorCanvas, SpacetimeCanvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U = uncompressed\n",
    "    # size of vector = shape of fiber\n",
    "    # contents = 0 if nothing in position, payload otherwise\n",
    "    # fibers serialized in position order\n",
    "uncompressed = \"U\"\n",
    "\n",
    "# Bu = untruncated bit vector\n",
    "    # size of vector = shape of this fiber\n",
    "    # contents = 0 in position if empty, 1 if not\n",
    "    # when each rank is serialized, fibers are serialized in order\n",
    "untruncated_bitvector = \"Bu\"\n",
    "\n",
    "# Bt = truncated bit vector\n",
    "    # cut off bit vector at last 1, store number of bits in previous rank's payloads\n",
    "    # size of vector <= shape of fiber\n",
    "    # when each rank is serialized, fibers are serialized in order\n",
    "truncated_bitvector = \"Bt\"\n",
    "\n",
    "# C = coordinate list\n",
    "    # size of vector = occupancy of this fiber\n",
    "    # contents = sorted / deduplicated coordinates in this fiber\n",
    "    # when each rank is serialized, fibers are serialized in order\n",
    "coord_list = \"C\"\n",
    "\n",
    "# list of all valid formats\n",
    "valid_formats =  [uncompressed, coord_list, untruncated_bitvector, truncated_bitvector] \n",
    "# [\"U\", \"C\", \"R\", \"A\", \"B\", \"D\", \"Hf\", \"Hr\"]\n",
    "\n",
    "# types of bitvectors\n",
    "bitvectors = [untruncated_bitvector, truncated_bitvector]\n",
    "\n",
    "# TO BE IMPLEMENTED\n",
    "# D = delta compressed\n",
    "    # num elements in vector = occupancy of fiber\n",
    "    # contents = delta-compressed coordinate list\n",
    "    # serialize according to position order\n",
    "# Hf = hash table per fiber\n",
    "    # TODO\n",
    "\n",
    "# compression format at some rank determines coordinate representation at that rank\n",
    "# coordinates at this rank stored explicitly as metadata\n",
    "explicit_coords = [coord_list, untruncated_bitvector, truncated_bitvector]\n",
    "\n",
    "# coordinates at this rank are stored implicitly\n",
    "implicit_coords = list(set(valid_formats) - set(explicit_coords))\n",
    "\n",
    "# compression format at a rank determines *payload* representation at previous rank\n",
    "# payloads at previous rank are implicit if sizeof(coords) == shape because you can index into it given the shape metadata\n",
    "# need explicit payloads iff size is proportional to occupancy\n",
    "implicit_payloads = [uncompressed, untruncated_bitvector]\n",
    "explicit_payloads = list(set(valid_formats) - set(implicit_payloads))\n",
    "\n",
    "class Codec:\n",
    "    # format descriptor should be a tuple of valid formats\n",
    "    # order descriptor specified SoA or AoS at each rank (currently unused)\n",
    "    # AoS / SoA doesn't apply to some formats (e.g. U) -> C (SoA, default should be here) / Ca (AoS)\n",
    "    def __init__(self, format_descriptor):\n",
    "        # check \n",
    "        for fmt in format_descriptor:\n",
    "            assert fmt in valid_formats\n",
    "        self.format_descriptor = format_descriptor\n",
    "        \n",
    "        # assumes pre-flattened for now\n",
    "        self.num_ranks = len(format_descriptor)\n",
    "                 \n",
    "    def get_format_descriptor(self):\n",
    "        return self.format_descriptor\n",
    "                 \n",
    "    def get_num_ranks(self):\n",
    "        return self.num_ranks\n",
    "    \n",
    "    # encode an explicit coordinate\n",
    "    # fmt = format at this rank, coords_key = key to output dict, prev_ind = pos+1 after previous nz,\n",
    "    # ind = position of current nz, output = output dict\n",
    "    # output = zeroes added to coords (e.g. in bit vector)\n",
    "    def encode_coord(self, fmt, coords_key, prev_ind, ind, output):\n",
    "        # if format at this level is C, store coords explicitly\n",
    "        elts_added = 0\n",
    "        if fmt is coord_list:\n",
    "            output[coords_key].append(ind)\n",
    "        if fmt in bitvectors:\n",
    "            for i in range(prev_ind, ind):\n",
    "                output[coords_key].append(0)\n",
    "                elts_added = elts_added + 1\n",
    "            output[coords_key].append(1)\n",
    "        return elts_added\n",
    "    \n",
    "    # given a tensor in HFA, encode according  to the format descriptor   \n",
    "    # depth start at -1, a = tensor, ranks = rank names lower, output = output dict\n",
    "    def encode(self, depth, a, ranks, output):\n",
    "        # keys are in the form payloads_{rank name}, coords_{rank name}\n",
    "        payloads_key = \"payloads_{}\".format(ranks[depth].lower())\n",
    "        coords_key = \"coords_{}\".format(ranks[depth].lower())\n",
    "        \n",
    "        # deal with the root separately\n",
    "        if depth == -1:           \n",
    "            # recurse one level down without adding to output yet\n",
    "            size = self.encode(depth + 1, a, ranks, output)\n",
    "            \n",
    "            # store at most one payload at the root (size of first rank)\n",
    "            if self.format_descriptor[depth + 1] in explicit_payloads:\n",
    "                payloads_key = \"payloads_root\"\n",
    "                output[payloads_key].append(size)\n",
    "                \n",
    "            return None              \n",
    "        \n",
    "        fmt = self.format_descriptor[depth]\n",
    "        dim_len = a.getShape()[0]\n",
    "        \n",
    "        # leaf level\n",
    "        if depth == self.num_ranks - 1:\n",
    "            # keep track of the occupancy of this fiber \n",
    "            occupancy = 0\n",
    "            \n",
    "            # if U, may have to add some zeroes, so we need indexing\n",
    "            prev_payloads_nz = 0\n",
    "            prev_coords_nz = 0\n",
    "            \n",
    "            # iterate nonzeroes in the fiber\n",
    "            for ind, (val) in a:\n",
    "                assert isinstance(val, Payload)\n",
    "                \n",
    "                # if coords are implicit, add zeroes between nzs\n",
    "                if fmt in implicit_coords:\n",
    "                   for i in range(prev_payloads_nz, ind):\n",
    "                       output[payloads_key].append(0)\n",
    "                   prev_payloads_nz = ind + 1\n",
    "                    \n",
    "                # output leaf-level payloads at nzs\n",
    "                output[payloads_key].append(val.value)\n",
    "                \n",
    "                # if this rank has explicit coords\n",
    "                if fmt in explicit_coords:\n",
    "                    added = self.encode_coord(fmt, coords_key, prev_coords_nz, ind, output)\n",
    "                    occupancy = occupancy + added\n",
    "                    prev_coords_nz = ind + 1\n",
    "                    \n",
    "                # count nzs in fiber\n",
    "                occupancy = occupancy + 1            \n",
    "            \n",
    "            # if coords are implicit, fill in zeroes at end of payloads\n",
    "            if self.format_descriptor[depth] in implicit_coords:\n",
    "                for i in range(prev_payloads_nz, dim_len):\n",
    "                    output[payloads_key].append(0)\n",
    "        \n",
    "            # if untruncated bitvector, fill in zeroes at end of coords\n",
    "            if self.format_descriptor[depth] is untruncated_bitvector:                \n",
    "                for i in range(prev_coords_nz, dim_len):\n",
    "                    output[coords_key].append(0)\n",
    "                    \n",
    "            return occupancy\n",
    "                \n",
    "        # internal levels\n",
    "        else:\n",
    "            next_fmt = self.format_descriptor[depth + 1]\n",
    "            \n",
    "            # keep track of occupancy of children and at current height\n",
    "            cumulative_occupancy = 0\n",
    "            fiber_occupancy = 0\n",
    "            prev_nz = 0\n",
    "\n",
    "            # if coords at this depth are implicit, recurse on *every* coordinate (may be empty)\n",
    "            if fmt in implicit_coords: \n",
    "                for i in range(0, dim_len):\n",
    "                    child_occupancy = self.encode(depth + 1, a.getPayload(i), ranks, output)\n",
    "                    \n",
    "                    # keep track of actual occupancy\n",
    "                    if not a.getPayload(i).isEmpty():\n",
    "                        fiber_occupancy = fiber_occupancy + 1\n",
    "                    \n",
    "                    # whether there are payloads here depends on the format of the next rank\n",
    "                    if next_fmt not in implicit_payloads:\n",
    "                        cumulative_occupancy = cumulative_occupancy + child_occupancy\n",
    "                        output[payloads_key].append(cumulative_occupancy)\n",
    "                        \n",
    "            # if coords at this depth are explicit, only the nonzeroes appear at lower ranks             \n",
    "            else:\n",
    "                # iterate through nonzeroes at this rank\n",
    "                for ind, (val) in a:\n",
    "                    assert isinstance(val, Fiber)\n",
    "                    # keep track of nonzeroes in this fiber\n",
    "                    fiber_occupancy = fiber_occupancy + 1\n",
    "\n",
    "                    # recursive call to sub-fibers (DFS traversal)\n",
    "                    child_occupancy = self.encode(depth + 1, val, ranks, output)\n",
    "\n",
    "                    # if this level needs to store the payloads explicitly\n",
    "                    # depends on the next level format: if coords are proportional to nnz\n",
    "                    if next_fmt not in implicit_payloads:\n",
    "                        cumulative_occupancy = cumulative_occupancy + child_occupancy\n",
    "                        output[payloads_key].append(cumulative_occupancy)   \n",
    "                    \n",
    "                    # store coords explicitly at this rank\n",
    "                    if fmt in explicit_coords:\n",
    "                        added = self.encode_coord(fmt, coords_key, prev_nz, ind, output)\n",
    "                        fiber_occupancy = fiber_occupancy + added\n",
    "                        prev_nz = ind + 1\n",
    "                \n",
    "                # if bitvector is untruncated, add zeroes to the end\n",
    "                if fmt is untruncated_bitvector:\n",
    "                    for i in range(prev_nz, dim_len):\n",
    "                        output[coords_key].append(0)\n",
    "                        \n",
    "            return fiber_occupancy\n",
    "    \n",
    "    # After encode\n",
    "    def decode(self, tensor):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth start for encode\n",
    "depth_start = -1\n",
    "\n",
    "# generate blank output dict based on rank names\n",
    "def get_output_dict(rank_names):\n",
    "    output = dict()\n",
    "    output[\"payloads_root\"] = []\n",
    "\n",
    "    for name in rank_names:\n",
    "        coords_key = \"coords_{}\".format(name.lower())\n",
    "        payloads_key = \"payloads_{}\".format(name.lower())\n",
    "\n",
    "        output[coords_key] = []\n",
    "        output[payloads_key] = []  \n",
    "    return output\n",
    "\n",
    "# given a tensor, descriptor, and dict of tensor encoded in that format\n",
    "# print and write out yaml in that format\n",
    "# TODO: change the output file name (currently just writes it to [descriptor string].yaml)\n",
    "def write_yaml(tensor, descriptor, tensor_in_format):\n",
    "    # header\n",
    "    header = dict()\n",
    "    header[\"name\"] = \"tensor-a\" # TODO: take this as input later\n",
    "    header[\"rank_ids\"] = tensor.getRankIds()\n",
    "    header[\"shapes\"] = tensor.getShape()\n",
    "    header[\"formats\"] = descriptor\n",
    "    rank_names = tensor.getRankIds()\n",
    "\n",
    "    # hierarchical yaml according to ranks\n",
    "    scratchpads = dict()\n",
    "    if len(tensor_in_format[\"payloads_root\"]) > 0:\n",
    "        scratchpads[\"root\"] = { \"payloads\" : tensor_in_format[\"payloads_root\"] }\n",
    "    \n",
    "    # write one rank at a time\n",
    "    for i in range(0, len(rank_names)):\n",
    "        rank_name = rank_names[i].lower()\n",
    "        coords_key = \"coords_{}\".format(rank_name)\n",
    "        payloads_key = \"payloads_{}\".format(rank_name)\n",
    "        key = \"rank_\" + str(i)\n",
    "        rank_dict = dict()\n",
    "        \n",
    "        # only write if scratchpad is nonempty\n",
    "        if len(tensor_in_format[coords_key]) > 0:\n",
    "            rank_dict[\"coords\"] = tensor_in_format[coords_key]\n",
    "        if len(tensor_in_format[payloads_key]) > 0:\n",
    "            rank_dict[\"payloads\"] = tensor_in_format[payloads_key]\n",
    "            \n",
    "        if len(rank_dict) > 0:\n",
    "            scratchpads[key] = rank_dict\n",
    "            \n",
    "    header[\"scratchpads\"] = scratchpads\n",
    "        \n",
    "    data = dict()\n",
    "    data[\"tensor\"] = header\n",
    "    outfilename = ''.join(descriptor) + '.yaml'\n",
    "\n",
    "    with open(outfilename, \"w\") as f:\n",
    "        print(yaml.dump(data, default_flow_style=None, sort_keys=False))\n",
    "        yaml.dump(data, f)\n",
    "\n",
    "# given a tensor and format descriptor, write the yaml for that format\n",
    "def try_format(tensor, descriptor):\n",
    "    print(\"\\n\" + str(descriptor))\n",
    "    codec = Codec(tuple(descriptor))\n",
    "        \n",
    "    # get output dict based on rank names\n",
    "    rank_names = small_mtx.getRankIds()\n",
    "        \n",
    "    # TODO: move output dict generation into codec\n",
    "    output = get_output_dict(rank_names)\n",
    "        \n",
    "    codec.encode(depth_start, tensor.getRoot(), tensor.getRankIds(), output)\n",
    "    \n",
    "    # remove empty scratchpads\n",
    "    write_yaml(tensor, descriptor, output)      \n",
    "        \n",
    "# generate all codecs\n",
    "def try_all_formats(tensor, possible_formats, descriptor, depth):\n",
    "    # once we have built a full descriptor, try it\n",
    "    if depth == len(tensor.getRankIds()):\n",
    "        try_format(tensor, descriptor)  \n",
    "\n",
    "    else:\n",
    "        # add a format to the descriptor and recurse\n",
    "        for format in formats:\n",
    "            temp = copy.deepcopy(descriptor)\n",
    "            temp.append(format)\n",
    "            try_all_formats(tensor, possible_formats, temp, depth + 1)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "formats = [\"U\", \"C\"]\n",
    "\n",
    "# run through all formats\n",
    "small_mtx_data = [[0, 1], [2, 3]]\n",
    "ranks = [\"M\", \"K\"]\n",
    "small_mtx = Tensor.fromUncompressed(ranks, small_mtx_data)\n",
    "\n",
    "# UU, UC\n",
    "try_format(small_mtx, [\"U\", \"U\"])\n",
    "try_format(small_mtx, [\"U\", \"C\"])\n",
    "\n",
    "# CU\n",
    "small_mtx_data = [[0, 0], [0, 3]]\n",
    "small_mtx = Tensor.fromUncompressed(ranks, small_mtx_data)\n",
    "try_format(small_mtx, [\"C\", \"U\"])\n",
    "try_format(small_mtx, [\"U\", \"U\"])\n",
    "\n",
    "# CC\n",
    "small_mtx_data = [[0, 0], [2, 3]]\n",
    "small_mtx = Tensor.fromUncompressed(ranks, small_mtx_data)\n",
    "try_format(small_mtx, [\"C\", \"C\"])\n",
    "try_format(small_mtx, [\"U\", \"U\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UxBu demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = [\"U\", \"Bu\"]\n",
    "\n",
    "# make small test tensor\n",
    "small_mtx_data = [[0, 0], [0, 0], [0, 3]]\n",
    "ranks = [\"M\", \"K\"]\n",
    "small_mtx = Tensor.fromUncompressed(ranks, small_mtx_data)\n",
    "\n",
    "# run through all formats\n",
    "try_all_formats(small_mtx, formats, [], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = [\"Bu\", \"Bt\"]\n",
    "\n",
    "# make small test tensor\n",
    "small_mtx_data = [[0, 0], [0, 1], [2, 0], [0, 0]]\n",
    "ranks = [\"M\", \"K\"]\n",
    "small_mtx = Tensor.fromUncompressed(ranks, small_mtx_data)\n",
    "\n",
    "# run through all formats\n",
    "try_all_formats(small_mtx, formats, [], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CxBu demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = [\"C\", \"Bu\"]\n",
    "\n",
    "# make small test tensor\n",
    "small_mtx_data = [[0, 0], [0, 3]]\n",
    "ranks = [\"M\", \"K\"]\n",
    "small_mtx = Tensor.fromUncompressed(ranks, small_mtx_data)\n",
    "\n",
    "# run through all formats\n",
    "try_all_formats(small_mtx, formats, [], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignore past here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small 3d example\n",
    "\n",
    "small_3d_data = [[[0, 1], [1, 0]],[[0, 1], [1, 0]],[[0, 1], [1, 0]],[[0, 1], [1, 0]]]\n",
    "small_3d = Tensor.fromUncompressed([\"W\", \"L\", \"H\"], small_3d_data)\n",
    "\n",
    "test_3d = Codec((\"U\", \"U\", \"U\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    def encode(self, tensor):\n",
    "        assert len(tensor.getRankIds()) == self.num_ranks\n",
    "        \n",
    "        # lower rank ids for loop gen\n",
    "        rank_ids = tensor.getRankIds()\n",
    "        lower_rank_ids = [rank_ids[i].lower() for i in range(0, len(rank_ids))]\n",
    "        print(lower_rank_ids)\n",
    "        \n",
    "        # write the header\n",
    "        \n",
    "        #auto generate loop nest\n",
    "        a = tensor\n",
    "        loops = \"a_{} = a.getRoot()\\n\".format(lower_rank_ids[0])\n",
    "\n",
    "        for i in  range(0, len(lower_rank_ids)):\n",
    "            cur_rank = lower_rank_ids[i]\n",
    "            if i < self.num_ranks - 1:\n",
    "                next_rank = lower_rank_ids[i+1]\n",
    "                loops = loops + \"for {}, (a_{}) in a_{}:\\n\".format(cur_rank, next_rank, cur_rank)    \n",
    "            else:\n",
    "                loops = loops + \"for {}, (a_val) in a_{}:\\n\".format(cur_rank, cur_rank)\n",
    "            loops = loops + \"\\t\" * (i+1)\n",
    "            \n",
    "        # for now, just print the coords\n",
    "        ranks = \",\".join(lower_rank_ids)\n",
    "        str = \"print(f\\\"(({{ {0} }}), {{a_val}})\\\")\".format(ranks)\n",
    "        loops = loops + str\n",
    "        print(loops)\n",
    "        # run it\n",
    "        exec(loops)\n",
    "        \n",
    "        a=\"def x():\\n\\t print(42)\\n\"\n",
    "        \n",
    "        exec(a, globals())\n",
    "        x()\n",
    "\n",
    "        return None\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
